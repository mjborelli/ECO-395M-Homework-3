---
title: "Data_Mining Homework 3"
author: "Matthew Borelli"
date: "4/14/2020"
output: md_document
---

```{r setup, include=FALSE}
knitr::opts_knit$set(root.dir = 'D:/Documents/MA Econ/Spring/Data Mining and Statistical Learning/ECO-395M-Homework-3/data')

library(dplyr)
library(plyr)
library(ggplot2)
library(randomForest)
library(stargazer)
library(foreach)
```

# Predictive Model Building

```{r green_data, include=FALSE}
green = read.csv("./greenbuildings.csv")
```

## Introduction

There has been an increased focus on creating environmentally conscious buildings, also known as green buildings, which adds a wrinkle into the already complex decision-making process for commercial real estate firms on what types of buildings they should construct. There are a variety of potential benefits that could make investing in constructing green buildings a worthwhile decision:

 - Lower operational costs (water, climate control, waste management, etc.).
 - Better indoor environments (natural sunlight for instance) could encourage better productivity and lead to happier and more motivated employees, increasing the incentives for a business to rent out those spaces.
 - Increased PR for both the real estate firm and the business renting the space due to positive public perceptions of green buildings.
 - Green buildings potentially have longer lives of operation. They are both physically constructed to last longer and less susceptible to energy market shocks.
 
 While this list of benefits seems to make an airtight case for commercial real estate firms to fully switch to constructing green buildings, there is a major issue. Green buildings are generally more costly for these firms to construct as a result of the standards that must be met in order to achieve green building certification from LEED or EnergyStar. However, as described above, it is possibly true that business would be willing to pay hire rents for office space in green buildings. This "Green Premium" would increase the profit incentives for commercial real estate firms to construct green buildings, which could be considered a societal good. However, we can't be certain that the "Green Premium" exists, or if so to what degree having a green certified building would increase rent prices. This analysis has two main goals:
 
 1. Create the best predictive model possible for rent prices.
 2. Use said model to quantify the average change in rental income per sq. ft. associated with attaining green certification, holding any other features of the building constant.
 
 Completing these two goals will give us a tool that we can employ to predict rent prices for buildings given certain features, as well as demonstrate to commercial real estate firms whether attempting to focus future constructions projects on green buildings is financially worth it. Given the uncertainty of the potential positive features listed above, this is a good quantifiable method for providing evidence for or against the impact of green certification on these companies' decision making processes.
 
## Data and Model

For this analysis, we are using using a data set of 7894 commercial rental properties in the U.S. Of these, 695 are certified as green buildings through either LEED or EnergyStar. In this data set, each green building is matched with a cluster of nearby non-green commercial buildings. Data is collected for a variety of features about the buildings, including rent in dollars per square foot, total square footage, age of building, etc. We will use these features in order to best predict building prices and quantify how much of a "Green Premium" exists. To start, we want to first look at a summary of the differences between different green classifications to give us a reference point for future results.

```{r green_summary, echo=FALSE}
summary_green_1 = as.data.frame(ddply(green, c("LEED", "Energystar"), summarise,
                                   N = length(Rent),
                                   "Rent" = mean(Rent),
                                   "Leasing Rate" = mean(leasing_rate),
                                   "Age" = mean(age),
                                   "Class A %" = round(mean(class_a)*100, 2),
                                   "Class B %" = round(mean(class_b)*100, 2)))
summary_green_1

summary_green_2 = as.data.frame(ddply(green, c("LEED", "Energystar"), summarise,
                                   "Temp Control Days" = mean(total_dd_07),
                                   "% w/ Amenities" = round(mean(amenities)*100, 2),
                                   "Precipitation" = mean(Precipitation)))
summary_green_2

summary_green_3 = as.data.frame(ddply(green, c("LEED", "Energystar"), summarise,
                                   "Gas Costs" = mean(Gas_Costs),
                                   "Electricity Costs" = mean(Electricity_Costs)))
summary_green_3
```

There are a lot of observations to make from these summary tables. A simple glance at the rent column shows that rent appears to be higher in green certified buildings than in non-green certified buildings. However, there also seems to be a big difference in the rental rate between EnergyStar certified buildings and LEED certified buildings, `r (summary_green_1[2,4] - summary_green_1[3,4])`$ per square foot. Looking closer at these tables, there are large differences in feature variables amongst the different populations of building types. The starkest difference is in days of temperature control, defined as the number of days where the building needs heating or cooling. EnergyStar buildings need `r (summary_green_2[1,3] - summary_green_2[2,3])` days fewer of heating or cooling than buildings with no green certification, but LEED buildings actually need `r (summary_green_2[3,3] - summary_green_2[1,3])` days more. 

These differences necessitate more formal evaluations of the data. Particularly, we will evaluate the effect of certification separately for LEED and EnergyStar, as buildings that meet their respective certifications seem to have different standards. To predict building rent prices  we will utilize random forest regression  First, we split the data into training and testing data, then use random forests to pick the model with the lowest error. For robustness, we will test the error for different numbers of trees in the random forest. However, we can't use random forests to quantify the effect of green certification on rent prices, as large tree and forests are generally not interpretable in the way we want. Therefore, we will utilize regression model selection techniques to best estimate rent. To check robustness, we will compare the simple model of rent regressed onto green certification with the model identified through our selection process on out-of-sample performance. Lastly, we will be controlling rent by cluster, by normalizing rent values using the clustered rent values.

```{r normalize rent, include = FALSE}
# The following two comments I wrote at the start of this process, keeping them here for posterity
# If this works, I'll be particularly proud of this
# Trying to normalize the rent variable by cluster, as cluster rents are highly correlated with raw rent values and potentially biasing the results

# The following doesn't work without this adhoc
green$cluster_se = 1
green$norm_rent = 1

# Part 1: This segment takes the subset from the greenbuildings data set for each cluster and calculates the standard error of rent for that cluster
cluster_se = data.frame("Cluster" = as.integer(), "SD" = as.numeric())
for (i in unique(green$cluster)) {
        x = subset(green, cluster == i)
        cluster_se [i, 1] = i
        cluster_se [i, 2] = sd(x$Rent)
}

# Part 2: This segment iteratively applies the cluster's standard error back into the original data set. Probably an inneficient way of doing this, would like to know how to do this better.
for (j in unique(green$cluster)) {
        for (i in 1:7894) {
                if (green[i,2] == cluster_se[j, 1]) {
                        green[i, 24] = cluster_se[j, 2]
                }
        }
}

# Part 3: Create a normalized rent value (the easy part!!)
green$norm_rent = (green$Rent - green$cluster_rent) / green$cluster_se

# Issue: 4 missing values, will manually fix them
# first, identify which observations have missing values
for (i in 1:7894) {
    if(is.na(green[i, 25]) == "TRUE") {
        print(i)
    }
}

# Through data exploration,there are missing values for normalized rent. These values consist of buildings that are the only listed building in their cluster, or buildings in a cluster where all buildings share the same rent value. Since I'm now using cluster-standardized errors, I have to drop these observations

green = green[-c(817, 1016, 1017, 1234, 1235, 1968, 2155, 2867, 2868, 3009, 3010, 3011, 3012, 3013, 3014, 3898, 3899, 5202, 5219, 5220, 5357, 5358, 5654, 5655, 5656, 5684, 5685, 6680, 6681, 6682, 6683, 6684, 6685, 6686, 6687, 6688, 6689, 6690, 6691, 6932, 7453, 7454),]
```

## Results

### Random Forest Model Errors
```{r green_forest_eval, include=FALSE}
# Setting a seed for consistent results
set.seed(14)

# Establish Train/Test split
N = nrow(green)
train_frac = 0.8
N_train = floor(train_frac*N)
N_test = N - N_train
train_ind = sample.int(N, N_train, replace=FALSE) %>% sort
green_train = green[train_ind,]
green_test = green[-train_ind,]

# Random Forest
forest_green = randomForest(Rent ~ size + leasing_rate + stories + age + renovated + class_a + class_b + LEED + Energystar + LEED*Energystar + net + amenities + total_dd_07 + Precipitation + Gas_Costs + Electricity_Costs, mtry=1, nTree=100, data=green_train)
plot(forest_green)
```
The model errors are high at low amounts of trees, flattening out at around 100 trees, so we will use 100 trees in our random forest model.

### Variable Importance Plot

Random Forests, as well as other large tree regression methods, are generally not interpretable. However, we can create measures of variable importance to help understand which feature variables by looking at which variables best improve error within the aggregated trees. Below is a variable importance plot, ranked in order of importance.

```{r, green_importance, echo = FALSE}
varImpPlot(forest_green)
```

Some of the most important variables are closely related since electricity costs, total days of heating or cooling required, precipitation amounts, and gas costs are all related to the running costs of a building. Interestingly, some of the least important variables are the green certification dummy variables, with EnergyStar being slightly more important than LEED. This is potentially evidence that having certification as a green building doesn't have a large impact on rental rates, but to be sure we will now check using stepwise regression model selection.

### Green Premium

To start the stepwise selection process, we introduce a null model of rent regressed onto the green building certification variables.

```{r simple_green_model, echo = FALSE}
simple_green = glm(norm_rent ~ LEED + Energystar + Energystar*LEED, data=green)
stargazer(simple_green, type="text")
```

FIX THIS MATT In this regression, we get positive coefficients for each of the green certification variables, but only EnergyStar certification's effect is statistically significant. LEED does not have any significantly positive effect on rent in our simple model. To compare, the following is the model picked by stepwise selection using AIC as the loss function.

```{r green_step, include = FALSE}
green_step = step(simple_green, 
			scope=~(. +size + leasing_rate + stories + age + renovated + class_a + class_b  + net + amenities + total_dd_07 + Precipitation + Gas_Costs + Electricity_Costs))
```

```{r green_final_model, echo = FALSE}
final_green = glm(norm_rent ~ LEED + Energystar + class_a + size + class_b + Gas_Costs + 
    net + age + leasing_rate + amenities, data = green)

stargazer(final_green, type="text")
```

In our final regression, we get positive significant values for LEED and Energystar certification on our normalized rents. This means that LEED certification is associated with a 0.352 standard deviation increase in rent above the local cluster's average rent and that Energystar certification is associated with a 0.179 standard deviation increase in rent above the local cluster's average rent.

## Conclusion

With the two models described above, we have created two models: a random forest for predicting price, the other a regression selection model quantifying the average change in rental income per square foot above average. 

In the random forest model, the most important variables are feature variables related to running costs, such as electricity costs, gas costs, and total days of heating or cooling needed. Relatively less important in our predictions are green certification (LEED and EnergyStar), amenities, and utility pay structure (the variable "net"). Of course, these less important variables are binary variables, which means it's less likely for them to reduce loss in these models anyways. Overall, it seems that rent prices are generally related to the running costs of a building, and less so on specific features of the commercial buildings.

Our regression selection method produced a model that regressed the cluster-normalized rent prices on a variety of feature variables, including LEED and EnergyStar certification. We took the cluster-normalized rents in this model in order to control for more expensive areas while avoiding multicollinearity issues that would arise from simply including clusterd rents in the regression model. Our results show that LEED and EnergyStar green certification are associated with increases in cluster-normalized rent prices, holding all else constant. This means that there is a "Green Premium" where buildings with green certifications have higher rents than those that don't, even if they have the same features. For future research, it would be prudent to access data that distinctifies the specific rating levels for LEED and EnergyStar green certification. Regression discontinuity methods could then be utilized to determine if there is just a sharp increase in rent prices, or if there is also a fuzzy increase after achieving certification that continues to increase rents. If there is simply a sharp increase, then the rent increase that we see isn't necessarily tied to the score given to a building, but just the actual distiction between being certified and not being certified.

# What Causes What?

##  Question 1.
#### Why can’t I just get data from a few different cities and run the regression of “Crime” on “Police” to understand how more cops in the streets affect crime? (“Crime” refers to some measure of crime rate and “Police” measures the number of cops in a city.)

You can't just run a regression of "amount of crime" on "amount of police" because cities with high rates of crime have a clear incentive to have more police officers. Since the amount of police officers is usually based on the amount of crime, running OLS of "Crime" on "Police" would get a positive coefficient, meaning that more police are associated with more crime. This might literally be true, but not the causal relationship we actually want to address.

## Question 2.
#### How were the researchers from UPenn able to isolate this effect? Briefly describe their approach and discuss their result in the “Table 2” below, from the researchers' paper.
The UPenn researchers got around the exogeneity problem mentioned in question 1 by studying crime rates in D.C. as they relate to the Terrorism Alert System (TAS). Since D.C. is a likely target of terrorist attacks, on days where the TAS is at "High Alert" level, more police are stationed on the National Mall. This large change in active police officers in the area is unrelated to the amount of normal (i.e. non-terrorism) crime in that area. Therefore, the researchers run two regressions

1. "Total number of crimes in D.C." regressed onto "High Alert" 
2. "Total number of crimes in D.C." regressed onto "High Alert" and "log midday metro ridership"

Regression 1 found that high-alert days are associated with a 7.316 decrease in crimes per day with 10% significancelevel. Regression 2 found that high-alert days are associated a 6.046 decrease in crimes per day at a 10% significance level, and a 10% increase in midday metro ridership is associated with a 1.7341 increase in daily crimes at a 5% significance level. Since high-alert days have exogenously higher amounts of police, we estimate that increases in active police officers are associated with decreases in daily crime in D.C.

## Question 3.
#### Why did they have to control for Metro ridership? What was that trying to capture?

The authors controlled for Metro ridership to hold constant the amount of people in Washington D.C. on high-alert days. They did this to test the concern that on high-alert days less people are on the National Mall. Having fewer people on the mall, basically meaning that there are less potential victims avaible, would mean that crime rates would fall for a reason outside of the increased police forces. Therefore, controlling for Metro ridership removes that causality from the error term, giving us a more accurate estimate of the effect of police on crime.

## Question 4.
#### Below I am showing you "Table 4" from the researchers' paper. Just focus on the first column of the table. Can you describe the model being estimated here? What is the conclusion?

In this model, they have split crime amounts into two categories: crime in District 1 and crime in all other districts. District 1 in Washington D.C. contains the National Mall, where most extra security is placed during high-alert days. In the results, we see that while crime in all other districts of D.C. dont experience any significant decrease during high-alert days, crime in district 1 does at the 5% significance level, controlling for metro ridership. This is stronger evidence for the effect of police on crime, as the district with the most additional police experiences the strongestdecline in daily crime rates.

# Clustering and PCA

```{r wine_data, include=FALSE}
wine = read.csv("./wine.csv")
```

## Introduction

How can we best determine what color a wine is? Can we tell anything about the quality of that wine based on its chemical properties. In this section, we will explore these questions using clustering and principal component analysis. We want to see if we 

## Data and Model

``` {r wine_null, echo=FALSE}
# To set up null model, we have to see how many of each there are

wine_red_null = sum(wine$color == "red")
wine_white_null = sum(wine$color == "white")

wine_null_accuracy = round(100*wine_white_null/(wine_white_null + wine_red_null), 2)
```

In order to evaluate clustering and principal component analysis models for determing wine color, we want to first set up a null model. In the data set, there are `r wine_red_null` observations of red wine and `r wine_white_null` observations of white wine. Since our model has more white wine observations than red, the null model predicts white wine for each observation. The null model has a `r wine_null_accuracy`% accuracy, the baseline that we will compare the other models to.

## Results

### K-Means Clustering

```{r wine_cluster_1, echo=FALSE}
wine_scale = wine[, -(12:13)]
wine_scale = scale(wine_scale, center=TRUE, scale=TRUE)

# Extract the centers and scales from the rescaled data (which are named attributes)
mu = attr(wine_scale , "scaled:center")
sigma = attr(wine_scale , "scaled:scale")

# Run k-means with 2 clusters ((hopefully white and red) and 25 starts
clust1 = kmeans(wine_scale, 2, nstart=25)
```
```{r wine_clust_1_display, echo=FALSE}
clust1$center[1,]*sigma + mu
```
```{r wine_clust_2_display, echo=FALSE}
clust1$center[2,]*sigma + mu
```

```{r whats_in_cluster, include=FALSE}
which(clust1$cluster == 1)
which(clust1$cluster == 2)

#exploring errors of the cluster
clust1$withinss
sum(clust1$withinss)
clust1$tot.withinss
clust1$betweenss
```
```{r cluster_predictions, echo=FALSE}
#grab cluster assignments
clust_data = as.data.frame(clust1$cluster)
colnames(clust_data)[1] = "cluster"
clust_data = transform(clust_data, cluster = as.numeric(cluster))

wine['cluster'] = clust_data['cluster']

wine_clust_table = table(Cluster = wine$cluster, Color = wine$color)
wine_clust_table

cluster_accuracy = round(100*(wine_clust_table[1,1] + wine_clust_table[2,2]) / nrow(wine))
```

Clustering is increibly accurate, with a `r cluster_accuracy`% accruacy, giving clustering a lift of `r (cluster_accuracy / wine_null_accuracy)` over the null model.

### Principal Components Analysis

```{r wine_PCA, echo=FALSE}
#no pun intended for reduced wine
pc2 = prcomp(wine_scale, scale=TRUE, rank=2)
loadings = pc2$rotation
scores = pc2$x

qplot(scores[,1], scores[,2], color=wine$color, xlab='Component 1', ylab='Component 2' ) +
    theme_dark() + scale_color_manual(values=c("red", "white"))

```
```{r wine_pca_insides_1, echo=FALSE}
v_best = pc2$rotation[,1]
v_best
```
```{r wine_pca_insides_2, echo=FALSE}
v_second_best = pc2$rotation[,2]
v_second_best
```

```{r PCA_var, include=FALSE}
wine_var_bycomponent = apply(wine_scale, 2, var)
total_wine_var = sum(wine_var_bycomponent)

# dot product
alpha_best = wine_scale %*% v_best
alpha_second = wine_scale %*% v_second_best

explained_wine_var_1 = var(alpha_best)/total_wine_var
explained_wine_var_2 = var(alpha_second)/total_wine_var
```

```{r pca_quality}
qplot(scores[,1], scores[,2], color=wine$quality, xlab='Component 1', ylab='Component 2' ) +
    theme_dark()
```

## Conclusion

# Market Segmentation

```{r social_data, include=FALSE}
social = read.csv("./social_marketing.csv")
```

## Introduction

## Data and Model

```{r social_preprocessing, include=FALSE}

# First step of pre-processing is to translate raw numbers of tweets into frequencies

social_freq = social[,-(1)]/rowSums(social[,-(1)])

#Look at counts of spam and adult
summary(social_freq[,35:36])

# filtering out any accounts with accounts with more than 20% adult content as well as removing spam and adult columns

social_freq = subset(social_freq[social_freq$adult < 0.25 , -c(35:36)])
```

```{r cluster_social, include=FALSE}
library(LICORS)
social_elbow = data.frame(K = as.integer(), Within_Error = as.numeric())
# Homemade elbow plot!
for (i in 1:10) {
    clust_social = kmeanspp(social_freq, k = i, nstart = 25)
    social_elbow[i, 1] = i
    social_elbow[i, 2] = sum(clust_social$withinss)
}

ggplot(data=social_elbow, mapping = aes(x = K, y = Within_Error)) + 
    geom_line()

# The elbow seems to be at K = 6, so we will go with a K+ Means model with 6 clusters
```

## Results

```{r social_final_cluster}

# Originally wasn't going to, but we need to scale the data to get better intuitions into what these clusters represent

social_freq_scaled = scale(social_freq, center=TRUE, scale=TRUE)

clust6_social = kmeanspp(social_freq_scaled, k = 6, nstart = 25)

Social_Cluster_1 = clust6_social$center[1,]
Social_Cluster_2 = clust6_social$center[2,]
Social_cluster_3 = clust6_social$center[3,]
Social_Cluster_4 = clust6_social$center[4,]
Social_Cluster_5 = clust6_social$center[5,]
Social_cluster_6 = clust6_social$center[6,]
```



## Conclusion

